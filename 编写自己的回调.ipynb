{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latter-spirit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 486 ms (started: 2021-08-26 12:57:40 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 自动计算cell的计算时间\n",
    "%load_ext autotime\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='svg' #矢量图设置，让绘图更清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "manufactured-planner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin\tgit@github.com:ustchope/keras_developer_guides-.git (fetch)\n",
      "origin\tgit@github.com:ustchope/keras_developer_guides-.git (push)\n",
      "[main 9327668] 更新 #3  Aug 26, 2021\n",
      " 2 files changed, 248 insertions(+), 131 deletions(-)\n",
      " create mode 100644 \"\\347\\274\\226\\345\\206\\231\\350\\207\\252\\345\\267\\261\\347\\232\\204\\345\\233\\236\\350\\260\\203.ipynb\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To git@github.com:ustchope/keras_developer_guides-.git\n",
      "   6cd06bd..9327668  main -> main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.19 s (started: 2021-08-26 01:54:19 +08:00)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# 增加更新\n",
    "git add *.ipynb *.md\n",
    "\n",
    "git remote -v\n",
    "\n",
    "git commit -m '更新 #3  Aug 26, 2021'\n",
    "\n",
    "#git push origin master\n",
    "git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "missing-branch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.01 s (started: 2021-08-26 12:57:44 +08:00)\n"
     ]
    }
   ],
   "source": [
    "#设置使用的gpu\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "   \n",
    "    gpu0 = gpus[1] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-criticism",
   "metadata": {},
   "source": [
    "# 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-balance",
   "metadata": {},
   "source": [
    "回调是在训练、评估或推理期间自定义 Keras 模型行为的强大工具。 示例包括使用 TensorBoard 可视化训练进度和结果的 tf.keras.callbacks.TensorBoard，或在训练期间定期保存模型的 tf.keras.callbacks.ModelCheckpoint。\n",
    "\n",
    "在本指南中，您将了解 Keras 回调是什么、它可以做什么以及如何构建自己的回调。 我们提供了一些简单的回调应用程序演示来帮助您入门。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-battery",
   "metadata": {},
   "source": [
    "# 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "perfect-kitchen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 373 µs (started: 2021-08-26 12:57:49 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-cookbook",
   "metadata": {},
   "source": [
    "# Keras 回调概述\n",
    "所有回调都是 keras.callbacks.Callback 类的子类，并覆盖在训练、测试和预测的各个阶段调用的一组方法。 回调对于在训练期间查看模型的内部状态和统计数据很有用。\n",
    "\n",
    "您可以将回调列表（作为关键字参数回调）传递给以下模型方法：\n",
    "* keras.Model.fit()\n",
    "* keras.Model.evaluate()\n",
    "* keras.Model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-genetics",
   "metadata": {},
   "source": [
    "# 回调方法概述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-enhancement",
   "metadata": {},
   "source": [
    "## 全局方法\n",
    "* `on_(train|test|predict)_begin(self,logs=None) ` \n",
    "在拟合/评估/预测开始时调用。\n",
    "* `on_(train|test|predict)_end(self,logs=None)`  \n",
    "在拟合/评估/预测结束时调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-ordinary",
   "metadata": {},
   "source": [
    "## 用于训练/测试/预测的批处理级方法\n",
    "* `on_(train|test|predict)_batch_begin(self,batch,logs=None)`  \n",
    "在训练/测试/预测期间处理批次之前调用。\n",
    "* `on_(train|test|predict)_batch_end(self,batch,logs=None)`  \n",
    "在训练/测试/预测批次结束时调用。 在此方法中，日志是一个包含指标结果的字典。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-probability",
   "metadata": {},
   "source": [
    "## 周期级方法（仅限训练）\n",
    "* `on_epoch_begin(self, epoch, logs=None)`  \n",
    "在训练期间的一个纪元开始时调用。\n",
    "* `on_epoch_end(self, epoch, logs=None)`  \n",
    "在训练期间的 epoch 结束时调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-gallery",
   "metadata": {},
   "source": [
    "# 一个基本的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-heart",
   "metadata": {},
   "source": [
    "我们来看一个具体的例子。 首先，让我们导入 tensorflow 并定义一个简单的 Sequential Keras 模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "canadian-feeling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.4 ms (started: 2021-08-26 13:00:15 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Define the Keras model to add callbacks to\n",
    "def get_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(1, input_dim=784))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[\"mean_absolute_error\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-suite",
   "metadata": {},
   "source": [
    "然后，从 Keras 数据集 API 加载用于训练和测试的 MNIST 数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "built-eight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 591 ms (started: 2021-08-26 13:05:02 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Load example MNIST data and pre-process it\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "\n",
    "# Limit the data to 1000 samples\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-toddler",
   "metadata": {},
   "source": [
    "现在，定义一个简单的自定义回调来记录：\n",
    "* 拟合/评估/预测开始和结束时\n",
    "* 当每个纪元开始和结束时\n",
    "* 当每个训练批次开始和结束时\n",
    "* 每个评估（测试）批次开始和结束时\n",
    "* 当每个推理（预测）批次开始和结束时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nearby-doubt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.16 ms (started: 2021-08-26 13:10:04 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-palmer",
   "metadata": {},
   "source": [
    "让我们试试看："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "behavioral-composition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training; got log keys: []\n",
      "Start epoch 0 of training; got log keys: []\n",
      "...Training: start of batch 0; got log keys: []\n",
      "...Training: end of batch 0; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Training: start of batch 1; got log keys: []\n",
      "...Training: end of batch 1; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Training: start of batch 2; got log keys: []\n",
      "...Training: end of batch 2; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Training: start of batch 3; got log keys: []\n",
      "...Training: end of batch 3; got log keys: ['loss', 'mean_absolute_error']\n",
      "Start testing; got log keys: []\n",
      "...Evaluating: start of batch 0; got log keys: []\n",
      "...Evaluating: end of batch 0; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Evaluating: start of batch 1; got log keys: []\n",
      "...Evaluating: end of batch 1; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Evaluating: start of batch 2; got log keys: []\n",
      "...Evaluating: end of batch 2; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Evaluating: start of batch 3; got log keys: []\n",
      "...Evaluating: end of batch 3; got log keys: ['loss', 'mean_absolute_error']\n",
      "Stop testing; got log keys: ['loss', 'mean_absolute_error']\n",
      "End epoch 0 of training; got log keys: ['loss', 'mean_absolute_error', 'val_loss', 'val_mean_absolute_error']\n",
      "Stop training; got log keys: ['loss', 'mean_absolute_error', 'val_loss', 'val_mean_absolute_error']\n",
      "Start testing; got log keys: []\n",
      "...Evaluating: start of batch 0; got log keys: []\n",
      "...Evaluating: end of batch 0; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Evaluating: start of batch 1; got log keys: []\n",
      "...Evaluating: end of batch 1; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Evaluating: start of batch 2; got log keys: []\n",
      "...Evaluating: end of batch 2; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Evaluating: start of batch 3; got log keys: []\n",
      "...Evaluating: end of batch 3; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Evaluating: start of batch 4; got log keys: []\n",
      "...Evaluating: end of batch 4; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Evaluating: start of batch 5; got log keys: []\n",
      "...Evaluating: end of batch 5; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Evaluating: start of batch 6; got log keys: []\n",
      "...Evaluating: end of batch 6; got log keys: ['loss', 'mean_absolute_error']\n",
      "...Evaluating: start of batch 7; got log keys: []\n",
      "...Evaluating: end of batch 7; got log keys: ['loss', 'mean_absolute_error']\n",
      "Stop testing; got log keys: ['loss', 'mean_absolute_error']\n",
      "Start predicting; got log keys: []\n",
      "...Predicting: start of batch 0; got log keys: []\n",
      "...Predicting: end of batch 0; got log keys: ['outputs']\n",
      "...Predicting: start of batch 1; got log keys: []\n",
      "...Predicting: end of batch 1; got log keys: ['outputs']\n",
      "...Predicting: start of batch 2; got log keys: []\n",
      "...Predicting: end of batch 2; got log keys: ['outputs']\n",
      "...Predicting: start of batch 3; got log keys: []\n",
      "...Predicting: end of batch 3; got log keys: ['outputs']\n",
      "...Predicting: start of batch 4; got log keys: []\n",
      "...Predicting: end of batch 4; got log keys: ['outputs']\n",
      "...Predicting: start of batch 5; got log keys: []\n",
      "...Predicting: end of batch 5; got log keys: ['outputs']\n",
      "WARNING:tensorflow:Callback method `on_predict_batch_end` is slow compared to the batch time (batch time: 0.0013s vs `on_predict_batch_end` time: 0.0017s). Check your callbacks.\n",
      "...Predicting: start of batch 6; got log keys: []\n",
      "...Predicting: end of batch 6; got log keys: ['outputs']\n",
      "...Predicting: start of batch 7; got log keys: []\n",
      "...Predicting: end of batch 7; got log keys: ['outputs']\n",
      "Stop predicting; got log keys: []\n",
      "time: 6.37 s (started: 2021-08-26 13:12:54 +08:00)\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    verbose=0,\n",
    "    validation_split=0.5,\n",
    "    callbacks=[CustomCallback()],\n",
    ")\n",
    "\n",
    "res = model.evaluate(\n",
    "    x_test, y_test, batch_size=128, verbose=0, callbacks=[CustomCallback()]\n",
    ")\n",
    "\n",
    "res = model.predict(x_test, batch_size=128, callbacks=[CustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-detective",
   "metadata": {},
   "source": [
    "## 日志字典的使用\n",
    "`logs`字典包含损失值，以及批次或时期结束时的所有指标。 示例包括损失和平均绝对误差。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "extra-chest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up to batch 0, the average loss is   23.47.\n",
      "Up to batch 1, the average loss is  446.34.\n",
      "Up to batch 2, the average loss is  304.99.\n",
      "Up to batch 3, the average loss is  231.12.\n",
      "Up to batch 4, the average loss is  186.23.\n",
      "Up to batch 5, the average loss is  156.36.\n",
      "Up to batch 6, the average loss is  134.78.\n",
      "Up to batch 7, the average loss is  121.21.\n",
      "The average loss for epoch 0 is  121.21 and mean absolute error is    5.87.\n",
      "Up to batch 0, the average loss is    4.70.\n",
      "Up to batch 1, the average loss is    4.78.\n",
      "Up to batch 2, the average loss is    4.83.\n",
      "Up to batch 3, the average loss is    4.68.\n",
      "Up to batch 4, the average loss is    4.81.\n",
      "Up to batch 5, the average loss is    4.92.\n",
      "Up to batch 6, the average loss is    5.23.\n",
      "Up to batch 7, the average loss is    5.47.\n",
      "The average loss for epoch 1 is    5.47 and mean absolute error is    1.91.\n",
      "Up to batch 0, the average loss is    8.47.\n",
      "Up to batch 1, the average loss is    7.42.\n",
      "Up to batch 2, the average loss is    7.76.\n",
      "Up to batch 3, the average loss is    7.78.\n",
      "Up to batch 4, the average loss is    8.09.\n",
      "Up to batch 5, the average loss is    7.98.\n",
      "Up to batch 6, the average loss is    8.01.\n",
      "Up to batch 7, the average loss is    7.93.\n",
      "time: 962 ms (started: 2021-08-26 13:15:37 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\n",
    "            \"Up to batch {}, the average loss is {:7.2f}.\".format(batch, logs[\"loss\"])\n",
    "        )\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(\n",
    "            \"Up to batch {}, the average loss is {:7.2f}.\".format(batch, logs[\"loss\"])\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\n",
    "            \"The average loss for epoch {} is {:7.2f} \"\n",
    "            \"and mean absolute error is {:7.2f}.\".format(\n",
    "                epoch, logs[\"loss\"], logs[\"mean_absolute_error\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=2,\n",
    "    verbose=0,\n",
    "    callbacks=[LossAndErrorPrintingCallback()],\n",
    ")\n",
    "\n",
    "res = model.evaluate(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    batch_size=128,\n",
    "    verbose=0,\n",
    "    callbacks=[LossAndErrorPrintingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-karaoke",
   "metadata": {},
   "source": [
    "# self.model 属性的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-picture",
   "metadata": {},
   "source": [
    "除了在调用其中一个方法时接收日志信息外，回调还可以访问与当前轮次训练/评估/推理相关的模型：`self.model`。\n",
    "\n",
    "以下是您可以在回调中使用 `self.model` 执行的一些操作：\n",
    "* 设置 `self.model.stop_training = True` 立即中断训练。\n",
    "* 改变优化器的超参数（可用作 `self.model.optimizer`），例如 `self.model.optimizer.learning_rate`。\n",
    "* 以周期间隔保存模型。\n",
    "* 在每个时期结束时在几个测试样本上记录 `model.predict()` 的输出，以用作训练期间的健全性检查。\n",
    "* 在每个时期结束时提取中间特征的可视化，以监控模型随时间学习的内容。\n",
    "* 等等。\n",
    "\n",
    "让我们通过几个例子来看看它的实际效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-sperm",
   "metadata": {},
   "source": [
    "# Keras 回调应用示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-harmony",
   "metadata": {},
   "source": [
    "* 以最小的损失提前停止\n",
    "第一个示例显示了通过设置属性 `self.model.stop_training (boolean)` 创建的回调，在达到最小损失时停止训练。 或者，您可以提供一个参数耐心来指定在达到局部最小值后我们应该在停止之前等待多少个时期。\n",
    "\n",
    "`tf.keras.callbacks.EarlyStopping` 提供了更完整和通用的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "foreign-repository",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up to batch 0, the average loss is   32.81.\n",
      "Up to batch 1, the average loss is  439.82.\n",
      "Up to batch 2, the average loss is  300.86.\n",
      "Up to batch 3, the average loss is  227.67.\n",
      "Up to batch 4, the average loss is  183.90.\n",
      "The average loss for epoch 0 is  183.90 and mean absolute error is    8.07.\n",
      "Up to batch 0, the average loss is    6.94.\n",
      "Up to batch 1, the average loss is    7.09.\n",
      "Up to batch 2, the average loss is    6.70.\n",
      "Up to batch 3, the average loss is    6.33.\n",
      "Up to batch 4, the average loss is    6.12.\n",
      "The average loss for epoch 1 is    6.12 and mean absolute error is    2.00.\n",
      "Up to batch 0, the average loss is    5.09.\n",
      "Up to batch 1, the average loss is    5.01.\n",
      "Up to batch 2, the average loss is    4.84.\n",
      "Up to batch 3, the average loss is    4.84.\n",
      "Up to batch 4, the average loss is    4.81.\n",
      "The average loss for epoch 2 is    4.81 and mean absolute error is    1.77.\n",
      "Up to batch 0, the average loss is    7.59.\n",
      "Up to batch 1, the average loss is    7.68.\n",
      "Up to batch 2, the average loss is   10.40.\n",
      "Up to batch 3, the average loss is   17.34.\n",
      "Up to batch 4, the average loss is   32.10.\n",
      "The average loss for epoch 3 is   32.10 and mean absolute error is    4.55.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00004: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa4a015fb50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 807 ms (started: 2021-08-26 13:28:47 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"当损失达到最小值时停止训练，即损失停止减少。\n",
    "\n",
    "   参数：\n",
    "       patience：达到 min 后要等待的时代数。 在这个数量没有改进之后，训练停止。\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # 当损失不再是最小值时它等待的纪元数。\n",
    "        self.wait = 0\n",
    "        # 训练停止的时代。\n",
    "        self.stopped_epoch = 0\n",
    "        # 将最好的初始化为无穷大。\n",
    "        self.best = np.Inf\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # 如果当前结果更好（更少），则记录最佳权重。\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    steps_per_epoch=5,\n",
    "    epochs=30,\n",
    "    verbose=0,\n",
    "    callbacks=[LossAndErrorPrintingCallback(), EarlyStoppingAtMinLoss()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-wallpaper",
   "metadata": {},
   "source": [
    "## 学习率调度\n",
    "在这个例子中，我们展示了如何在训练过程中使用自定义回调来动态改变优化器的学习率。\n",
    "\n",
    "有关更一般的实现，请参阅 `callbacks.LearningRateScheduler`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "norwegian-ethernet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00000: Learning rate is 0.1000.\n",
      "Up to batch 0, the average loss is   27.36.\n",
      "Up to batch 1, the average loss is  496.30.\n",
      "Up to batch 2, the average loss is  336.32.\n",
      "Up to batch 3, the average loss is  254.64.\n",
      "Up to batch 4, the average loss is  205.18.\n",
      "The average loss for epoch 0 is  205.18 and mean absolute error is    8.30.\n",
      "\n",
      "Epoch 00001: Learning rate is 0.1000.\n",
      "Up to batch 0, the average loss is    6.55.\n",
      "Up to batch 1, the average loss is    6.30.\n",
      "Up to batch 2, the average loss is    5.92.\n",
      "Up to batch 3, the average loss is    6.13.\n",
      "Up to batch 4, the average loss is    6.01.\n",
      "The average loss for epoch 1 is    6.01 and mean absolute error is    2.03.\n",
      "\n",
      "Epoch 00002: Learning rate is 0.1000.\n",
      "Up to batch 0, the average loss is    6.92.\n",
      "Up to batch 1, the average loss is    6.09.\n",
      "Up to batch 2, the average loss is    6.02.\n",
      "Up to batch 3, the average loss is    5.98.\n",
      "Up to batch 4, the average loss is    5.62.\n",
      "The average loss for epoch 2 is    5.62 and mean absolute error is    1.89.\n",
      "\n",
      "Epoch 00003: Learning rate is 0.0500.\n",
      "Up to batch 0, the average loss is    3.38.\n",
      "Up to batch 1, the average loss is    3.48.\n",
      "Up to batch 2, the average loss is    3.57.\n",
      "Up to batch 3, the average loss is    3.74.\n",
      "Up to batch 4, the average loss is    3.77.\n",
      "The average loss for epoch 3 is    3.77 and mean absolute error is    1.56.\n",
      "\n",
      "Epoch 00004: Learning rate is 0.0500.\n",
      "Up to batch 0, the average loss is    3.63.\n",
      "Up to batch 1, the average loss is    3.62.\n",
      "Up to batch 2, the average loss is    3.88.\n",
      "Up to batch 3, the average loss is    3.82.\n",
      "Up to batch 4, the average loss is    3.88.\n",
      "The average loss for epoch 4 is    3.88 and mean absolute error is    1.54.\n",
      "\n",
      "Epoch 00005: Learning rate is 0.0500.\n",
      "Up to batch 0, the average loss is    6.09.\n",
      "Up to batch 1, the average loss is    5.97.\n",
      "Up to batch 2, the average loss is    7.37.\n",
      "Up to batch 3, the average loss is    7.89.\n",
      "Up to batch 4, the average loss is    8.43.\n",
      "The average loss for epoch 5 is    8.43 and mean absolute error is    2.41.\n",
      "\n",
      "Epoch 00006: Learning rate is 0.0100.\n",
      "Up to batch 0, the average loss is    9.75.\n",
      "Up to batch 1, the average loss is    7.98.\n",
      "Up to batch 2, the average loss is    6.12.\n",
      "Up to batch 3, the average loss is    5.49.\n",
      "Up to batch 4, the average loss is    5.10.\n",
      "The average loss for epoch 6 is    5.10 and mean absolute error is    1.78.\n",
      "\n",
      "Epoch 00007: Learning rate is 0.0100.\n",
      "Up to batch 0, the average loss is    2.88.\n",
      "Up to batch 1, the average loss is    3.39.\n",
      "Up to batch 2, the average loss is    3.75.\n",
      "Up to batch 3, the average loss is    3.66.\n",
      "Up to batch 4, the average loss is    3.64.\n",
      "The average loss for epoch 7 is    3.64 and mean absolute error is    1.50.\n",
      "\n",
      "Epoch 00008: Learning rate is 0.0100.\n",
      "Up to batch 0, the average loss is    3.20.\n",
      "Up to batch 1, the average loss is    3.25.\n",
      "Up to batch 2, the average loss is    3.43.\n",
      "Up to batch 3, the average loss is    3.48.\n",
      "Up to batch 4, the average loss is    3.42.\n",
      "The average loss for epoch 8 is    3.42 and mean absolute error is    1.45.\n",
      "\n",
      "Epoch 00009: Learning rate is 0.0050.\n",
      "Up to batch 0, the average loss is    3.64.\n",
      "Up to batch 1, the average loss is    3.28.\n",
      "Up to batch 2, the average loss is    3.15.\n",
      "Up to batch 3, the average loss is    2.93.\n",
      "Up to batch 4, the average loss is    3.14.\n",
      "The average loss for epoch 9 is    3.14 and mean absolute error is    1.39.\n",
      "\n",
      "Epoch 00010: Learning rate is 0.0050.\n",
      "Up to batch 0, the average loss is    2.41.\n",
      "Up to batch 1, the average loss is    2.78.\n",
      "Up to batch 2, the average loss is    2.90.\n",
      "Up to batch 3, the average loss is    3.04.\n",
      "Up to batch 4, the average loss is    3.29.\n",
      "The average loss for epoch 10 is    3.29 and mean absolute error is    1.38.\n",
      "\n",
      "Epoch 00011: Learning rate is 0.0050.\n",
      "Up to batch 0, the average loss is    3.62.\n",
      "Up to batch 1, the average loss is    3.48.\n",
      "Up to batch 2, the average loss is    3.32.\n",
      "Up to batch 3, the average loss is    3.28.\n",
      "Up to batch 4, the average loss is    3.11.\n",
      "The average loss for epoch 11 is    3.11 and mean absolute error is    1.41.\n",
      "\n",
      "Epoch 00012: Learning rate is 0.0010.\n",
      "Up to batch 0, the average loss is    3.80.\n",
      "Up to batch 1, the average loss is    3.95.\n",
      "Up to batch 2, the average loss is    3.53.\n",
      "Up to batch 3, the average loss is    3.50.\n",
      "Up to batch 4, the average loss is    3.66.\n",
      "The average loss for epoch 12 is    3.66 and mean absolute error is    1.51.\n",
      "\n",
      "Epoch 00013: Learning rate is 0.0010.\n",
      "Up to batch 0, the average loss is    3.77.\n",
      "Up to batch 1, the average loss is    3.63.\n",
      "Up to batch 2, the average loss is    3.37.\n",
      "Up to batch 3, the average loss is    3.31.\n",
      "Up to batch 4, the average loss is    3.34.\n",
      "The average loss for epoch 13 is    3.34 and mean absolute error is    1.41.\n",
      "\n",
      "Epoch 00014: Learning rate is 0.0010.\n",
      "Up to batch 0, the average loss is    2.72.\n",
      "Up to batch 1, the average loss is    2.76.\n",
      "Up to batch 2, the average loss is    2.99.\n",
      "Up to batch 3, the average loss is    3.15.\n",
      "Up to batch 4, the average loss is    2.95.\n",
      "The average loss for epoch 14 is    2.95 and mean absolute error is    1.33.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa4a00af0a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.41 s (started: 2021-08-26 13:42:31 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class CustomLearningRateScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"学习率调度器，根据调度设置学习率。\n",
    "\n",
    "   参数：\n",
    "       schedule：一个函数，它以纪元索引（整数，从 0 开始索引）\n",
    "       和当前学习率作为输入，并返回一个新的学习率作为输出（浮点数）。\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, schedule):\n",
    "        super(CustomLearningRateScheduler, self).__init__()\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # 从模型的优化器中获取当前的学习率。\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
    "        # 调用 schedule 函数来获取预定的学习率。\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # 在此 epoch 开始之前将值设置回优化器\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))\n",
    "\n",
    "\n",
    "LR_SCHEDULE = [\n",
    "    # (epoch to start, learning rate) tuples\n",
    "    (3, 0.05),\n",
    "    (6, 0.01),\n",
    "    (9, 0.005),\n",
    "    (12, 0.001),\n",
    "]\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
    "        return lr\n",
    "    for i in range(len(LR_SCHEDULE)):\n",
    "        if epoch == LR_SCHEDULE[i][0]:\n",
    "            return LR_SCHEDULE[i][1]\n",
    "    return lr\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    steps_per_epoch=5,\n",
    "    epochs=15,\n",
    "    verbose=0,\n",
    "    callbacks=[\n",
    "        LossAndErrorPrintingCallback(),\n",
    "        CustomLearningRateScheduler(lr_schedule),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-music",
   "metadata": {},
   "source": [
    "# 内置 Keras 回调\n",
    "请务必通过阅读 API 文档来查看现有的 Keras 回调。 应用程序包括记录到 CSV、保存模型、在 TensorBoard 中可视化指标等等！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-davis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
