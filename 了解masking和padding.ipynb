{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assumed-execution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 640 ms (started: 2021-08-26 16:44:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 自动计算cell的计算时间\n",
    "%load_ext autotime\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='svg' #矢量图设置，让绘图更清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# 增加更新\n",
    "git add *.ipynb *.md\n",
    "\n",
    "git remote -v\n",
    "\n",
    "git commit -m '更新 #6  Aug 26, 2021'\n",
    "\n",
    "#git push origin master\n",
    "git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "focused-luxembourg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.38 s (started: 2021-08-26 16:46:28 +08:00)\n"
     ]
    }
   ],
   "source": [
    "#设置使用的gpu\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "   \n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-device",
   "metadata": {},
   "source": [
    "# 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coated-guatemala",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1 ms (started: 2021-08-26 16:46:45 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-somalia",
   "metadata": {},
   "source": [
    "# 介绍\n",
    "`masking`是一种告诉序列处理层输入中的某些时间步丢失的方法，因此在处理数据时应该跳过。\n",
    "\n",
    "`padding`是一种特殊形式的掩码，其中掩码步骤位于序列的开头或结尾。 填充来自需要将序列数据编码成连续的批次：为了使批次中的所有序列适合给定的标准长度，需要填充或截断一些序列。\n",
    "\n",
    "让我们仔细看看。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-situation",
   "metadata": {},
   "source": [
    "# 填充序列数据\n",
    "在处理序列数据时，单个样本的长度不同是很常见的。 考虑以下示例（文本标记为单词）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "compact-reference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'world', '!'],\n",
       " ['How', 'are', 'you', 'doing', 'today'],\n",
       " ['The', 'weather', 'will', 'be', 'nice', 'tomorrow']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.1 ms (started: 2021-08-26 16:49:57 +08:00)\n"
     ]
    }
   ],
   "source": [
    "[\n",
    "  [\"Hello\", \"world\", \"!\"],\n",
    "  [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n",
    "  [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-directive",
   "metadata": {},
   "source": [
    "词汇查找后，数据可能被向量化为整数，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "  [71, 1331, 4231]\n",
    "  [73, 8, 3215, 55, 927],\n",
    "  [83, 91, 1, 645, 1253, 927],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-clearing",
   "metadata": {},
   "source": [
    "数据是一个嵌套列表，其中单个样本的长度分别为 3、5 和 6。 由于深度学习模型的输入数据必须是单个张量（在这种情况下形状为 (batch_size, 6, vocab_size)），比最长项目短的样本需要填充一些占位符值（或者，一个 也可能在填充短样本之前截断长样本）。\n",
    "\n",
    "Keras 提供了一个实用函数来截断和填充 Python 列表到一个公共长度：`tf.keras.preprocessing.sequence.pad_sequences`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "described-infection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 711  632   71    0    0    0]\n",
      " [  73    8 3215   55  927    0]\n",
      " [  83   91    1  645 1253  927]]\n",
      "time: 2.78 ms (started: 2021-08-26 16:53:57 +08:00)\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    [711, 632, 71],\n",
    "    [73, 8, 3215, 55, 927],\n",
    "    [83, 91, 1, 645, 1253, 927],\n",
    "]\n",
    "\n",
    "# 默认情况下，这将使用 0s 填充； 它可以通过“value”参数进行配置。\n",
    "# 请注意，您可以“预”填充（在开头）或“后”填充（在结尾）。\n",
    "# 我们建议在使用 RNN 层时使用“post”填充（为了能够使用层的 CuDNN 实现）。\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    raw_inputs, padding=\"post\"\n",
    ")\n",
    "print(padded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-institution",
   "metadata": {},
   "source": [
    "# Masking\n",
    "现在所有样本都有统一的长度，模型必须被告知数据的某些部分实际上是填充并且应该被忽略。 这种机制就是掩蔽。\n",
    "\n",
    "在 Keras 模型中引入输入掩码有以下三种方式：\n",
    "* 添加 keras.layers.Masking 层。\n",
    "* 使用 mask_zero=True 配置 keras.layers.Embedding 层。\n",
    "* 在调用支持此参数的层（例如 RNN 层）时手动传递掩码参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-pharmacy",
   "metadata": {},
   "source": [
    "# 掩码生成层：Embedding和Masking\n",
    "在幕后，这些层将创建一个掩码张量（形状为 (batch, sequence_length) 的二维张量），并将其附加到由掩码或嵌入层返回的张量输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "revised-teach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True False False False]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n",
      "time: 1.84 s (started: 2021-08-26 16:58:25 +08:00)\n"
     ]
    }
   ],
   "source": [
    "embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "masked_output = embedding(padded_inputs)\n",
    "\n",
    "print(masked_output._keras_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cordless-testimony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True False False False]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n",
      "time: 21 ms (started: 2021-08-26 17:00:46 +08:00)\n"
     ]
    }
   ],
   "source": [
    "masking_layer = layers.Masking()\n",
    "# 通过将 2D 输入扩展为 3D 来模拟嵌入查找，嵌入维度为 10。\n",
    "unmasked_embedding = tf.cast(\n",
    "    tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1, 1, 10]), tf.float32\n",
    ")\n",
    "\n",
    "masked_embedding = masking_layer(unmasked_embedding)\n",
    "print(masked_embedding._keras_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-knock",
   "metadata": {},
   "source": [
    "从打印结果中可以看出，掩码是一个形状为`(batch_size, sequence_length)` 的 2D 布尔张量，其中每个单独的 False 条目表示在处理过程中应忽略相应的时间步长。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-overhead",
   "metadata": {},
   "source": [
    "# 功能 API 和顺序 API 中的掩码传播\n",
    "使用 Functional API 或 Sequential API 时，由 Embedding 或 Masking 层生成的掩码将通过网络传播到能够使用它们的任何层（例如，RNN 层）。 Keras 将自动获取与输入对应的掩码，并将其传递给任何知道如何使用它的层。\n",
    "\n",
    "例如，在下面的 Sequential 模型中，LSTM 层将自动接收一个掩码，这意味着它将忽略填充值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "weighted-stranger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.18 s (started: 2021-08-26 17:03:02 +08:00)\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True), layers.LSTM(32),]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-summit",
   "metadata": {},
   "source": [
    "以下 Functional API 模型也是这种情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "compact-deadline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.3 s (started: 2021-08-26 17:03:22 +08:00)\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
    "outputs = layers.LSTM(32)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-organ",
   "metadata": {},
   "source": [
    "# 将掩码张量直接传递给图层\n",
    "可以处理掩码的层（例如 LSTM 层）在其 `__call__` 方法中有一个掩码参数。\n",
    "\n",
    "同时，生成掩码的层（例如嵌入）公开了一个您可以调用的 `compute_mask(input, previous_mask)` 方法。\n",
    "\n",
    "因此，您可以将掩码生成层的 `compute_mask()` 方法的输出传递给掩码消耗层的 `__call__` 方法，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "noticed-democrat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 32), dtype=float32, numpy=\n",
       "array([[ 2.10512639e-03,  7.91605655e-03,  2.91242613e-03, ...,\n",
       "         4.98896092e-03,  1.13983441e-03,  2.61795532e-04],\n",
       "       [ 2.21813447e-03, -1.06557585e-04,  8.73247348e-03, ...,\n",
       "        -6.83238730e-03, -3.40499380e-03,  5.75671019e-03],\n",
       "       [-6.44142088e-03,  1.45612126e-02,  7.02295825e-03, ...,\n",
       "         3.50034097e-04,  1.32520909e-05, -9.08983347e-04],\n",
       "       ...,\n",
       "       [-1.18978610e-02, -6.21891534e-03, -6.85542997e-04, ...,\n",
       "         3.14077246e-03,  4.66374028e-03,  6.02304656e-03],\n",
       "       [ 2.04775925e-03, -5.54631464e-03,  7.60610308e-03, ...,\n",
       "         3.86148854e-03, -1.55524036e-03,  7.93630164e-03],\n",
       "       [ 5.74535131e-03, -8.79379455e-03,  5.86576853e-03, ...,\n",
       "        -3.53310985e-04,  6.32018549e-03,  1.16598709e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.29 s (started: 2021-08-26 17:05:23 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "        self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "        self.lstm = layers.LSTM(32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        # 请注意，您也可以手动准备一个 `mask` 张量。\n",
    "        # 它只需要是一个布尔张量\n",
    "        # 使用正确的形状，即 (batch_size, timesteps)。\n",
    "        mask = self.embedding.compute_mask(inputs)\n",
    "        output = self.lstm(x, mask=mask)  # 该层将忽略屏蔽值\n",
    "        return output\n",
    "\n",
    "\n",
    "layer = MyLayer()\n",
    "x = np.random.random((32, 10)) * 100\n",
    "x = x.astype(\"int32\")\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-processing",
   "metadata": {},
   "source": [
    "# 支持自定义图层中的遮罩\n",
    "有时，您可能需要编写生成蒙版的图层（如嵌入），或需要修改当前蒙版的图层。\n",
    "\n",
    "例如，任何生成时间维度与其输入不同的张量的层，例如在时间维度上连接的 `Concatenate` 层，都需要修改当前掩码，以便下游层能够正确地将掩码时间步长转换为 帐户。\n",
    "\n",
    "为此，您的图层应实现 `layer.compute_mask()` 方法，该方法在给定输入和当前掩码的情况下生成一个新掩码。\n",
    "\n",
    "这是一个需要修改当前蒙版的 `TemporalSplit` 图层的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "honest-schedule",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]], shape=(3, 3), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[False False False]\n",
      " [ True  True False]\n",
      " [ True  True  True]], shape=(3, 3), dtype=bool)\n",
      "time: 8.78 ms (started: 2021-08-26 17:07:27 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class TemporalSplit(keras.layers.Layer):\n",
    "    \"\"\"将输入张量沿时间维度拆分为 2 个张量。\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 期望输入为 3D，掩码为 2D，沿时间轴（轴 1）将输入张量分成 2 个子张量。\n",
    "        return tf.split(inputs, 2, axis=1)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # 如果出现，也将mask分成 2 个。\n",
    "        if mask is None:\n",
    "            return None\n",
    "        return tf.split(mask, 2, axis=1)\n",
    "\n",
    "\n",
    "first_half, second_half = TemporalSplit()(masked_embedding)\n",
    "print(first_half._keras_mask)\n",
    "print(second_half._keras_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-advertising",
   "metadata": {},
   "source": [
    "这是一个 CustomEmbedding 层的另一个示例，它能够从输入值生成掩码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "affecting-alberta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[False False  True  True  True  True  True  True  True  True]\n",
      " [ True  True  True  True  True  True False  True  True  True]\n",
      " [ True False  True  True  True  True False  True  True False]], shape=(3, 10), dtype=bool)\n",
      "time: 17.7 ms (started: 2021-08-26 17:09:01 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class CustomEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\n",
    "        super(CustomEmbedding, self).__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.mask_zero = mask_zero\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self.input_dim, self.output_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            dtype=\"float32\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        return tf.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "layer = CustomEmbedding(10, 32, mask_zero=True)\n",
    "x = np.random.random((3, 10)) * 9\n",
    "x = x.astype(\"int32\")\n",
    "\n",
    "y = layer(x)\n",
    "mask = layer.compute_mask(x)\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-sponsorship",
   "metadata": {},
   "source": [
    "# 选择在兼容层上屏蔽传播\n",
    "大多数图层不修改时间维度，因此不需要修改当前蒙版。 然而，他们可能仍然希望能够将当前掩码不改变地传播到下一层。 这是一种选择加入的行为。 默认情况下，自定义层将销毁当前掩码（因为框架无法判断传播掩码是否安全）。\n",
    "\n",
    "如果您有一个不修改时间维度的自定义层，并且您希望它能够传播当前输入掩码，则应在层构造函数中设置 `self.supports_masking = True`。 在这种情况下，`compute_mask()` 的默认行为是只传递当前掩码。\n",
    "\n",
    "下面是一个被列入蒙版传播白名单的图层示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "drawn-thinking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 514 µs (started: 2021-08-26 17:09:45 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class MyActivation(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyActivation, self).__init__(**kwargs)\n",
    "        # Signal that the layer is safe for mask propagation\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-enough",
   "metadata": {},
   "source": [
    "您现在可以在蒙版生成层（如嵌入）和蒙版消耗层（如 LSTM）之间使用此自定义图层，它将传递蒙版，使其到达蒙版消耗层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "digital-earthquake",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask found: KerasTensor(type_spec=TensorSpec(shape=(None, None), dtype=tf.bool, name=None), name='Placeholder_1:0')\n",
      "time: 1.04 s (started: 2021-08-26 17:10:23 +08:00)\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
    "x = MyActivation()(x)  # Will pass the mask along\n",
    "print(\"Mask found:\", x._keras_mask)\n",
    "outputs = layers.LSTM(32)(x)  # Will receive the mask\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-fitting",
   "metadata": {},
   "source": [
    "# 编写需要mask信息的图层\n",
    "一些层是掩码消费者：他们在调用中接受`mask`参数并使用它来确定是否跳过某些时间步骤。\n",
    "\n",
    "要编写这样的层，您只需在调用签名中添加一个 `mask=None` 参数即可。 与输入关联的掩码将在可用时传递到您的图层。\n",
    "\n",
    "下面是一个简单的示例：在输入序列的时间维度（轴 1）上计算 softmax 的层，同时丢弃屏蔽的时间步长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "induced-executive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 133 ms (started: 2021-08-26 17:11:43 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class TemporalSoftmax(keras.layers.Layer):\n",
    "    def call(self, inputs, mask=None):\n",
    "        broadcast_float_mask = tf.expand_dims(tf.cast(mask, \"float32\"), -1)\n",
    "        inputs_exp = tf.exp(inputs) * broadcast_float_mask\n",
    "        inputs_sum = tf.reduce_sum(inputs * broadcast_float_mask, axis=1, keepdims=True)\n",
    "        return inputs_exp / inputs_sum\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "x = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs)\n",
    "x = layers.Dense(1)(x)\n",
    "outputs = TemporalSoftmax()(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "y = model(np.random.randint(0, 10, size=(32, 100)), np.random.random((32, 100, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-librarian",
   "metadata": {},
   "source": [
    "# 概括\n",
    "这就是您需要了解的有关 Keras 中的填充和遮罩的全部信息。 回顾一下：\n",
    "* “masking”是层如何知道何时跳过/忽略序列输入中的某些时间步。\n",
    "* 有些层是掩码生成器：`Embedding`可以从输入值（如果 mask_zero=True）生成掩码，`masking`层也可以。\n",
    "* 有些层是掩码消费者：他们在他们的 `__call__ `方法中公开了一个掩码参数。 这就是 RNN 层的情况。\n",
    "* 在 Functional API 和 Sequential API 中，掩码信息是自动传播的。\n",
    "* 以独立方式使用图层时，您可以手动将遮罩参数传递给图层。\n",
    "* 您可以轻松编写修改当前蒙版、生成新蒙版或使用与输入关联的蒙版的图层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-indicator",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
