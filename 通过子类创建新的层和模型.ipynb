{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "considered-christmas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 15.8 ms (started: 2021-08-25 15:02:45 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 自动计算cell的计算时间\n",
    "%load_ext autotime\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='svg' #矢量图设置，让绘图更清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informal-north",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin\tgit@github.com:ustchope/keras_developer_guides-.git (fetch)\n",
      "origin\tgit@github.com:ustchope/keras_developer_guides-.git (push)\n",
      "[main a24f968] 更新 #1  Aug 25, 2021\n",
      " 1 file changed, 1382 insertions(+), 1 deletion(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To git@github.com:ustchope/keras_developer_guides-.git\n",
      "   ce88496..a24f968  main -> main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.78 s (started: 2021-08-25 15:02:48 +08:00)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# 增加更新\n",
    "git add *.ipynb *.md\n",
    "\n",
    "git remote -v\n",
    "\n",
    "git commit -m '更新 #1  Aug 25, 2021'\n",
    "\n",
    "#git push origin master\n",
    "git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "owned-strategy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.54 s (started: 2021-08-25 15:02:53 +08:00)\n"
     ]
    }
   ],
   "source": [
    "#设置使用的gpu\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "   \n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-denmark",
   "metadata": {},
   "source": [
    "# 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stuck-bread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 840 µs (started: 2021-08-25 15:05:14 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-potential",
   "metadata": {},
   "source": [
    "# `Layer` 类：状态（权重）和一些计算的组合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-swaziland",
   "metadata": {},
   "source": [
    "Keras 中的核心抽象之一是 `Layer` 类。 层封装了状态（层的`weights`）和从输入到输出的转换（`call`，层的前向传递）。\n",
    "\n",
    "这是一个密集连接层。 它有一个状态：变量 `w` 和 `b`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equipped-criterion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.33 ms (started: 2021-08-23 18:11:04 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        \n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-storm",
   "metadata": {},
   "source": [
    "您可以通过在一些张量输入上调用层来使用它，就像 Python 函数一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "instant-hungarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.02051926 0.14575195 0.05476379 0.06521606]\n",
      " [0.02051926 0.14575195 0.05476379 0.06521606]], shape=(2, 4), dtype=float32)\n",
      "time: 3.23 s (started: 2021-08-23 18:11:50 +08:00)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-belfast",
   "metadata": {},
   "source": [
    "请注意，权重 `w` 和 `b` 在被设置为层属性后会被层自动跟踪："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "specified-limitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 754 µs (started: 2021-08-23 18:14:19 +08:00)\n"
     ]
    }
   ],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-necessity",
   "metadata": {},
   "source": [
    "请注意，您还可以使用更快的快捷方式为图层添加权重：add_weight() 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "delayed-stability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.04528809 -0.04564667 -0.01765442  0.0017395 ]\n",
      " [-0.04528809 -0.04564667 -0.01765442  0.0017395 ]], shape=(2, 4), dtype=float32)\n",
      "time: 15.2 ms (started: 2021-08-23 18:15:01 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-setting",
   "metadata": {},
   "source": [
    "# 层可以具有不可训练的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-polymer",
   "metadata": {},
   "source": [
    "除了可训练的权重，您还可以向层添加不可训练的权重。 当您训练该层时，在反向传播期间不应考虑此类权重。\n",
    "\n",
    "以下是添加和使用不可训练重量的方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "worse-quality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n",
      "time: 18.8 ms (started: 2021-08-23 18:16:47 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-kernel",
   "metadata": {},
   "source": [
    "# 最佳实践：推迟权重创建，直到知道输入的形状"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-organ",
   "metadata": {},
   "source": [
    "我们上面的 Linear 层采用 `input_dim` 参数，用于计算 `__init__()` 中权重 `w` 和 `b` 的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "continental-entrance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.11 ms (started: 2021-08-23 18:17:54 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(units,), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-trigger",
   "metadata": {},
   "source": [
    "在许多情况下，您可能事先不知道输入的大小，并且在实例化图层一段时间后，当该值已知时，您希望懒惰地创建权重。\n",
    "\n",
    "在 Keras API 中，我们建议在层的 build(self,input_shape) 方法中创建层权重。 像这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "frequent-maryland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.69 ms (started: 2021-08-23 18:18:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-bookmark",
   "metadata": {},
   "source": [
    "层的 `__call__()` 方法将在第一次调用时自动运行 `build`。 你现在有一个懒惰的层，因此更容易使用："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-douglas",
   "metadata": {},
   "source": [
    "# 层是可递归组合的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-dover",
   "metadata": {},
   "source": [
    "如果您将一个 Layer 实例指定为另一个 `Layer` 的属性，则外层将开始跟踪内层的权重。\n",
    "\n",
    "我们建议在 `__init__()` 方法中创建这样的子层（因为子层通常有一个 `build` 方法，它们将在构建外层时构建）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "optional-meditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable weights: 6\n",
      "time: 29.9 ms (started: 2021-08-23 18:21:36 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 假设我们正在重用 Linear 类\n",
    "# 使用我们上面定义的 `build` 方法。\n",
    "\n",
    "\n",
    "class MLPBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
    "print(\"weights:\", len(mlp.weights))\n",
    "print(\"trainable weights:\", len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-hanging",
   "metadata": {},
   "source": [
    "# add_loss() 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-drawing",
   "metadata": {},
   "source": [
    "在编写层的 `call()` 方法时，您可以创建稍后在编写训练循环时要使用的损失张量。 这可以通过调用 `self.add_loss(value)` 来实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ranking-gospel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.06 ms (started: 2021-08-23 18:24:13 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 创建活动正则化损失的层\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-dayton",
   "metadata": {},
   "source": [
    "这些损失（包括由任何内层创建的损失）可以通过 `layer.losses` 检索。 此属性在每次 `__call__()` 开始时重置到顶级层，因此 `layer.losses` 始终包含在上次前向传递期间创建的损失值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "precious-western",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.5 ms (started: 2021-08-23 18:26:11 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class OuterLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0  # 没有损失，因为该层从未被调用过\n",
    "\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # 我们创建了一个损失值\n",
    "\n",
    "# `layer.losses` 在每次 __call__ 开始时重置\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # 这是上面调用过程中产生的损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-talent",
   "metadata": {},
   "source": [
    "此外，`loss`属性还包含为任何内层的权重创建的正则化损失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "available-circuit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0018706928>]\n",
      "time: 31.1 ms (started: 2021-08-23 18:28:14 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayerWithKernelRegularizer, self).__init__()\n",
    "        self.dense = keras.layers.Dense(\n",
    "            32, kernel_regularizer=tf.keras.regularizers.l2(1e-3)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayerWithKernelRegularizer()\n",
    "_ = layer(tf.zeros((1, 1)))\n",
    "\n",
    "# This is `1e-3 * sum(layer.dense.kernel ** 2)`,\n",
    "# created by the `kernel_regularizer` above.\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-philadelphia",
   "metadata": {},
   "source": [
    "在编写训练循环时要考虑这些损失，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Iterate over the batches of a dataset.\n",
    "for x_batch_train, y_batch_train in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = layer(x_batch_train)  # Logits for this minibatch\n",
    "        # Loss value for this minibatch\n",
    "        loss_value = loss_fn(y_batch_train, logits)\n",
    "        # Add extra losses created during this forward pass:\n",
    "        loss_value += sum(model.losses)\n",
    "\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-intelligence",
   "metadata": {},
   "source": [
    "有关编写训练循环的详细指南，请参阅从头开始编写训练循环的指南。\n",
    "\n",
    "这些损失也可以与 fit() 无缝协作（它们会自动求和并添加到主要损失中，如果有的话）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "promotional-reform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 178ms/step - loss: 0.1220\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc94c155550>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 964 ms (started: 2021-08-23 18:32:40 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "outputs = ActivityRegularizationLayer()(inputs)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# 如果在 `compile` 中传递了损失，则正则化损失被添加到它\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n",
    "\n",
    "# 也可以在 `compile` 中不传递任何损失，\n",
    "# 因为模型已经有一个损失要最小化，通过`add_loss`\n",
    "# 在前传期间调用！\n",
    "model.compile(optimizer=\"adam\")\n",
    "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-british",
   "metadata": {},
   "source": [
    "# add_metric() 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-lease",
   "metadata": {},
   "source": [
    "与 `add_loss()` 类似，层也有一个 `add_metric()` 方法，用于在训练期间跟踪数量的移动平均值。\n",
    "\n",
    "考虑以下层：“物流端点”层。 它将预测和目标作为输入，计算通过 `add_loss()` 跟踪的损失，并计算通过 `add_metric()` 跟踪的准确度标量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "variable-universe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.85 ms (started: 2021-08-23 18:37:15 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "\n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # Log accuracy as a metric and add it\n",
    "        # to the layer using `self.add_metric()`.\n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_metric(acc, name=\"accuracy\")\n",
    "\n",
    "        # Return the inference-time prediction tensor (for `.predict()`).\n",
    "        return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-governor",
   "metadata": {},
   "source": [
    "以这种方式跟踪的指标可以通过 `layer.metrics` 访问："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "innocent-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.metrics: [<tensorflow.python.keras.metrics.BinaryAccuracy object at 0x7fc94c127d60>]\n",
      "current accuracy value: 1.0\n",
      "time: 55 ms (started: 2021-08-23 18:37:48 +08:00)\n"
     ]
    }
   ],
   "source": [
    "layer = LogisticEndpoint()\n",
    "\n",
    "targets = tf.ones((2, 2))\n",
    "logits = tf.ones((2, 2))\n",
    "y = layer(targets, logits)\n",
    "\n",
    "print(\"layer.metrics:\", layer.metrics)\n",
    "print(\"current accuracy value:\", float(layer.metrics[0].result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-brave",
   "metadata": {},
   "source": [
    "就像 `add_loss()` 一样，这些指标由 `fit()` 跟踪："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "protecting-stationery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 276ms/step - loss: 1.1473 - binary_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc92c14fd90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 475 ms (started: 2021-08-23 18:39:02 +08:00)\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
    "\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "model.compile(optimizer=\"adam\")\n",
    "\n",
    "data = {\n",
    "    \"inputs\": np.random.random((3, 3)),\n",
    "    \"targets\": np.random.random((3, 10)),\n",
    "}\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-brisbane",
   "metadata": {},
   "source": [
    "# 您可以选择在图层上启用序列化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-behavior",
   "metadata": {},
   "source": [
    "如果您需要将自定义层作为功能模型的一部分进行序列化，您可以选择实现 get_config() 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "median-championship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64}\n",
      "time: 3.77 ms (started: 2021-08-23 18:45:17 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"units\": self.units}\n",
    "\n",
    "\n",
    "# Now you can recreate the layer from its config:\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-breed",
   "metadata": {},
   "source": [
    "请注意，基础 `Layer` 类的 `__init__()` 方法采用一些关键字参数，特别是名称和数据类型。 将这些参数传递给 `__init__()` 中的父类并将它们包含在层配置中是一种很好的做法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "governing-queue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'linear_7', 'trainable': True, 'dtype': 'float32', 'units': 64}\n",
      "time: 3.82 ms (started: 2021-08-23 18:47:13 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n",
    "\n",
    "\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-installation",
   "metadata": {},
   "source": [
    "如果在从其配置反序列化层时需要更大的灵活性，您还可以覆盖 `from_config()` 类方法。 这是 `from_config()` 的基本实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_config(cls, config):\n",
    "    return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-browse",
   "metadata": {},
   "source": [
    "要了解有关序列化和保存的更多信息，请参阅保存和序列化模型的完整指南。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-minority",
   "metadata": {},
   "source": [
    "# call() 方法中的优先训练参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-calculation",
   "metadata": {},
   "source": [
    "某些层，尤其是 `BatchNormalization` 层和 `Dropout` 层，在训练和推理过程中具有不同的行为。 对于此类层，标准做法是在 `call()` 方法中公开训练（布尔值）参数。\n",
    "\n",
    "通过在 `call()` 中公开此参数，您可以启用内置的训练和评估循环（例如 `fit()`）以在训练和推理中正确使用该层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "characteristic-moment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.19 ms (started: 2021-08-23 18:49:24 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class CustomDropout(keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-worst",
   "metadata": {},
   "source": [
    "# call() 方法中的优先掩码参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-topic",
   "metadata": {},
   "source": [
    "`call()` 支持的另一个特权参数是`mask`参数。\n",
    "\n",
    "您会在所有 Keras RNN 层中找到它。 掩码是一个布尔张量（输入中每个时间步长一个布尔值），用于在处理时间序列数据时跳过某些输入时间步长。\n",
    "\n",
    "当前一层生成掩码时，Keras 将自动将正确的`mask`参数传递给支持它的层的 `__call__()` 。 生成`mask`的层是配置`mask_zero=True`的`Embedding`层和`Masking`层。\n",
    "\n",
    "要了解有关掩码以及如何编写启用遮罩的图层的更多信息，请查看指南“了解填充和掩码”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-reservation",
   "metadata": {},
   "source": [
    "# 模型类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-texas",
   "metadata": {},
   "source": [
    "通常，您将使用 `Layer` 类来定义内部计算块，并使用 `Model` 类来定义外部模型——您将训练的对象。\n",
    "\n",
    "例如，在 ResNet50 模型中，您将有多个 ResNet 块子类化层，以及一个包含整个 ResNet50 网络的模型。\n",
    "\n",
    "`Model` 类与 `Layer` 具有相同的 API，但有以下区别：\n",
    "* 它公开了内置的训练、评估和预测循环（`model.fit()`、`model.evaluate()`、`model.predict()`）。\n",
    "* 它通过 `model.layers` 属性公开其内部层的列表。\n",
    "* 它公开了保存和序列化 API（`save()`、`save_weights()`...）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-dallas",
   "metadata": {},
   "source": [
    "实际上，`Layer` 类对应于我们在文献中所说的“层”（如“卷积层”或“循环层”）或“块”（如“ResNet 块”或“初始块”） ）。\n",
    "\n",
    "同时，`Model` 类对应于文献中所谓的“模型”（如“深度学习模型”）或“网络”（如“深度神经网络”）。\n",
    "\n",
    "因此，如果您想知道，“我应该使用 `Layer` 类还是 `Model` 类？”，请问自己：我是否需要对其调用 `fit()` ？ 我需要调用 `save()` 吗？ 如果是这样，请选择模型。 如果不是（因为您的类只是更大系统中的一个块，或者因为您正在编写培训和保存代码），请使用 `Layer`。\n",
    "\n",
    "例如，我们可以采用上面的 mini-resnet 示例，并使用它来构建一个模型，我们可以使用 `fit()` 训练该模型，并且可以使用 `save_weights()` 进行保存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_1 = ResNetBlock()\n",
    "        self.block_2 = ResNetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "resnet = ResNet()\n",
    "dataset = ...\n",
    "resnet.fit(dataset, epochs=10)\n",
    "resnet.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-inside",
   "metadata": {},
   "source": [
    "# 把它们放在一起：一个端到端的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-necklace",
   "metadata": {},
   "source": [
    "以下是您到目前为止所学到的：\n",
    "* 一个层封装了一个状态（在 `__init__()` 或 `build()` 中创建）和一些计算（在 `call()` 中定义）。\n",
    "* 层可以递归嵌套以创建新的、更大的计算块。\n",
    "* 层可以通过 `add_loss()` 和 `add_metric()` 创建和跟踪损失（通常是正则化损失）以及度量\n",
    "* 外层容器，你要训练的东西，是一个模型。 模型就像一个层，但增加了训练和序列化实用程序。\n",
    "\n",
    "让我们把所有这些东西放在一个端到端的例子中：我们将实现一个变分自动编码器 (VAE)。 我们将在 MNIST 数字上训练它。\n",
    "\n",
    "我们的 VAE 将是 `Model` 的子类，构建为子类 `Layer` 的嵌套层组合。 它将具有正则化损失（KL 散度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "clinical-rebate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.3 ms (started: 2021-08-25 15:05:27 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "#         batch, dim = z_mean.shape\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "#         epsilon = tf.random_normal(shape=(batch,dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "    \n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "    \n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        original_dim,\n",
    "        intermediate_dim=64,\n",
    "        latent_dim=32,\n",
    "        name=\"autoencoder\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-soccer",
   "metadata": {},
   "source": [
    "让我们在 MNIST 上编写一个简单的训练循环："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tough-midnight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = 0.3232\n",
      "step 100: mean loss = 0.1249\n",
      "step 200: mean loss = 0.0989\n",
      "step 300: mean loss = 0.0890\n",
      "step 400: mean loss = 0.0841\n",
      "step 500: mean loss = 0.0808\n",
      "step 600: mean loss = 0.0787\n",
      "step 700: mean loss = 0.0771\n",
      "step 800: mean loss = 0.0759\n",
      "step 900: mean loss = 0.0749\n",
      "Start of epoch 1\n",
      "step 0: mean loss = 0.0746\n",
      "step 100: mean loss = 0.0740\n",
      "step 200: mean loss = 0.0735\n",
      "step 300: mean loss = 0.0730\n",
      "step 400: mean loss = 0.0727\n",
      "step 500: mean loss = 0.0723\n",
      "step 600: mean loss = 0.0720\n",
      "step 700: mean loss = 0.0717\n",
      "step 800: mean loss = 0.0714\n",
      "step 900: mean loss = 0.0712\n",
      "time: 29.7 s (started: 2021-08-25 15:07:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-franchise",
   "metadata": {},
   "source": [
    "请注意，由于 VAE 是模型的子类，它具有内置的训练循环。 所以你也可以这样训练它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "incorrect-filter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "938/938 [==============================] - 9s 9ms/step - loss: 0.0747\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 9s 9ms/step - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc92c066070>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 18.4 s (started: 2021-08-23 19:19:42 +08:00)\n"
     ]
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-raising",
   "metadata": {},
   "source": [
    "# 超越面向对象的开发：函数式 API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-neighbor",
   "metadata": {},
   "source": [
    "这个例子对你来说是不是太多面向对象的开发？ 您还可以使用 Functional API 构建模型。 重要的是，选择一种或另一种风格并不妨碍您利用以另一种风格编写的组件：您始终可以混合搭配。\n",
    "\n",
    "例如，下面的函数式 API 示例重用了我们在上面的示例中定义的相同采样层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "optional-tennessee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.0745\n",
      "Epoch 2/3\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0676\n",
      "Epoch 3/3\n",
      "938/938 [==============================] - 9s 10ms/step - loss: 0.0675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0508016b80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 29.4 s (started: 2021-08-25 15:09:35 +08:00)\n"
     ]
    }
   ],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# Train.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-bedroom",
   "metadata": {},
   "source": [
    "有关更多信息，请务必阅读 Functional API 指南。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-italian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
