{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accomplished-victory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 735 ms (started: 2021-08-26 19:21:23 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 自动计算cell的计算时间\n",
    "%load_ext autotime\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='svg' #矢量图设置，让绘图更清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occasional-obligation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin\tgit@github.com:ustchope/keras_developer_guides-.git (fetch)\n",
      "origin\tgit@github.com:ustchope/keras_developer_guides-.git (push)\n",
      "[main 98ea9e4] 更新 #8  Aug 26, 2021\n",
      " 5 files changed, 3067 insertions(+), 38 deletions(-)\n",
      " create mode 100644 \"\\344\\275\\277\\347\\224\\250 TensorFlow Cloud \\350\\256\\255\\347\\273\\203 Keras \\346\\250\\241\\345\\236\\213.ipynb\"\n",
      " create mode 100644 \"\\345\\244\\232 GPU \\345\\222\\214\\345\\210\\206\\345\\270\\203\\345\\274\\217\\350\\256\\255\\347\\273\\203.ipynb\"\n",
      " create mode 100644 \"\\350\\277\\201\\347\\247\\273\\345\\255\\246\\344\\271\\240\\345\\222\\214\\345\\276\\256\\350\\260\\203.ipynb\"\n",
      " create mode 100644 \"\\351\\235\\242\\345\\220\\221\\347\\240\\224\\347\\251\\266\\344\\272\\272\\345\\221\\230\\347\\232\\204 Keras \\347\\256\\200\\344\\273\\213.ipynb\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To git@github.com:ustchope/keras_developer_guides-.git\n",
      "   93164f7..98ea9e4  main -> main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.14 s (started: 2021-08-26 19:22:03 +08:00)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# 增加更新\n",
    "git add *.ipynb *.md\n",
    "\n",
    "git remote -v\n",
    "\n",
    "git commit -m '更新 #8  Aug 26, 2021'\n",
    "\n",
    "#git push origin master\n",
    "git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loaded-pierre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.41 s (started: 2021-08-26 19:22:09 +08:00)\n"
     ]
    }
   ],
   "source": [
    "#设置使用的gpu\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "   \n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    # 或者也可以设置GPU显存为固定使用量(例如：4G)\n",
    "    #tf.config.experimental.set_virtual_device_configuration(gpu0,\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) \n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-murray",
   "metadata": {},
   "source": [
    "# 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opposed-bread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 498 µs (started: 2021-08-26 19:22:33 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-dakota",
   "metadata": {},
   "source": [
    "# 介绍\n",
    "你是机器学习研究员吗？ 您是否在 NeurIPS 上发表文章并推动 CV 和 NLP 的最新技术？ 本指南将作为您对核心 Keras 和 TensorFlow API 概念的首次介绍。\n",
    "\n",
    "在本指南中，您将了解：\n",
    "* TensorFlow 中的张量、变量和梯度\n",
    "* 通过子类化 Layer 类来创建层\n",
    "* 编写低级训练循环\n",
    "* 通过 add_loss() 方法跟踪由层创建的损失\n",
    "* 在低级训练循环中跟踪指标\n",
    "* 使用编译的 tf.function 加速执行\n",
    "* 在训练或推理模式下执行层\n",
    "* Keras 函数式 API\n",
    "\n",
    "您还将在两个端到端研究示例中看到 Keras API 的实际应用：变分自动编码器和超网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-nickname",
   "metadata": {},
   "source": [
    "# 张量\n",
    "TensorFlow 是用于可微分编程的基础设施层。 从本质上讲，它是一个用于操作 N 维数组（张量）的框架，很像 NumPy。\n",
    "\n",
    "但是，NumPy 和 TensorFlow 之间存在三个主要区别：\n",
    "* TensorFlow 可以利用硬件加速器，例如 GPU 和 TPU。\n",
    "* TensorFlow 可以自动计算任意可微张量表达式的梯度。\n",
    "* TensorFlow 计算可以分布在一台机器上的大量设备上，也可以分布在大量机器上（每个机器可能有多个设备）。\n",
    "\n",
    "让我们来看看 TensorFlow 的核心对象：Tensor。\n",
    "\n",
    "这是一个常数张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aerial-confirmation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5 2]\n",
      " [1 3]], shape=(2, 2), dtype=int32)\n",
      "time: 1.71 s (started: 2021-08-26 19:23:59 +08:00)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[5, 2], [1, 3]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-accessory",
   "metadata": {},
   "source": [
    "您可以通过调用 .numpy() 以 NumPy 数组的形式获取其值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "listed-marine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2],\n",
       "       [1, 3]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 14.3 ms (started: 2021-08-26 19:24:20 +08:00)\n"
     ]
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-buddy",
   "metadata": {},
   "source": [
    "很像 NumPy 数组，它具有属性 dtype 和 shape："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "collected-emission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: <dtype: 'int32'>\n",
      "shape: (2, 2)\n",
      "time: 734 µs (started: 2021-08-26 19:24:39 +08:00)\n"
     ]
    }
   ],
   "source": [
    "print(\"dtype:\", x.dtype)\n",
    "print(\"shape:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-discharge",
   "metadata": {},
   "source": [
    "创建常量张量的常用方法是通过 tf.ones 和 tf.zeros（就像 np.ones 和 np.zeros）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cathedral-baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]], shape=(2, 1), dtype=float32)\n",
      "time: 17 ms (started: 2021-08-26 19:25:01 +08:00)\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones(shape=(2, 1)))\n",
    "print(tf.zeros(shape=(2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-office",
   "metadata": {},
   "source": [
    "您还可以创建随机常数张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deadly-detection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 19.3 ms (started: 2021-08-26 19:25:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)\n",
    "\n",
    "x = tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-congo",
   "metadata": {},
   "source": [
    "# 变量\n",
    "变量是用于存储可变状态（例如神经网络的权重）的特殊张量。 您使用一些初始值创建一个变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "trying-chester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-1.0114193 , -0.8392619 ],\n",
      "       [-0.14134249,  0.508994  ]], dtype=float32)>\n",
      "time: 10.2 ms (started: 2021-08-26 19:25:55 +08:00)\n"
     ]
    }
   ],
   "source": [
    "initial_value = tf.random.normal(shape=(2, 2))\n",
    "a = tf.Variable(initial_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-dylan",
   "metadata": {},
   "source": [
    "您可以使用 `.assign(value)`、`.assign_add(increment)` 或 `.assign_sub(decrement)` 方法更新变量的值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "blank-probe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 40.4 ms (started: 2021-08-26 19:26:40 +08:00)\n"
     ]
    }
   ],
   "source": [
    "new_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign(new_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j]\n",
    "\n",
    "added_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign_add(added_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j] + added_value[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-melissa",
   "metadata": {},
   "source": [
    "# 在 TensorFlow 中进行数学运算\n",
    "如果您使用过 NumPy，那么在 TensorFlow 中进行数学运算看起来会非常熟悉。 主要区别在于您的 TensorFlow 代码可以在 GPU 和 TPU 上运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "drawn-waste",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12.1 ms (started: 2021-08-26 19:27:04 +08:00)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "c = a + b\n",
    "d = tf.square(c)\n",
    "e = tf.exp(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-eclipse",
   "metadata": {},
   "source": [
    "# 梯度\n",
    "这是与 NumPy 的另一个重大区别：您可以自动检索任何可微表达式的梯度。\n",
    "\n",
    "只需打开一个 `GradientTape`，通过`tape.watch()` 开始“观察”一个张量，并使用这个张量作为输入组成一个可微的表达式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "promotional-musician",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.15414332  0.41588616]\n",
      " [ 0.98600286 -0.69903207]], shape=(2, 2), dtype=float32)\n",
      "time: 19.9 ms (started: 2021-08-26 19:27:42 +08:00)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)  # Start recording the history of operations applied to `a`\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))  # Do some math using `a`\n",
    "    # What's the gradient of `c` with respect to `a`?\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-reply",
   "metadata": {},
   "source": [
    "默认情况下，变量是自动监视的，因此您无需手动监视它们："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "incorporated-floating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.15414332  0.41588616]\n",
      " [ 0.98600286 -0.69903207]], shape=(2, 2), dtype=float32)\n",
      "time: 14.2 ms (started: 2021-08-26 19:28:05 +08:00)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(a)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "    dc_da = tape.gradient(c, a)\n",
    "    print(dc_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-sculpture",
   "metadata": {},
   "source": [
    "请注意，您可以通过嵌套磁带来计算高阶导数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "indian-determination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.5860508  1.0559046 ]\n",
      " [0.01313475 0.30708778]], shape=(2, 2), dtype=float32)\n",
      "time: 23 ms (started: 2021-08-26 19:28:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as outer_tape:\n",
    "    with tf.GradientTape() as tape:\n",
    "        c = tf.sqrt(tf.square(a) + tf.square(b))\n",
    "        dc_da = tape.gradient(c, a)\n",
    "    d2c_da2 = outer_tape.gradient(dc_da, a)\n",
    "    print(d2c_da2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-bibliography",
   "metadata": {},
   "source": [
    "# Keras 层\n",
    "TensorFlow 是用于可微编程的基础设施层，处理张量、变量和梯度，而 Keras 是用于深度学习的用户界面，处理层、模型、优化器、损失函数、指标等。\n",
    "\n",
    "Keras 作为 TensorFlow 的高级 API：Keras 使 TensorFlow 变得简单而高效。\n",
    "\n",
    "Layer 类是 Keras 中的基本抽象。 一个层封装了一个状态（权重）和一些计算（在调用方法中定义）。\n",
    "\n",
    "一个简单的层看起来像这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "exclusive-backup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 888 µs (started: 2021-08-26 19:29:27 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-latvia",
   "metadata": {},
   "source": [
    "您可以像使用 Python 函数一样使用 Layer 实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "blind-ballet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.32 s (started: 2021-08-26 19:30:15 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate our layer.\n",
    "linear_layer = Linear(units=4, input_dim=2)\n",
    "\n",
    "# The layer can be treated as a function.\n",
    "# Here we call it on some data.\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert y.shape == (2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-director",
   "metadata": {},
   "source": [
    "权重变量（在 `__init__` 中创建）在 `weights` 属性下自动跟踪："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "arabic-toilet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 399 µs (started: 2021-08-26 19:30:55 +08:00)\n"
     ]
    }
   ],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-warner",
   "metadata": {},
   "source": [
    "您有许多可用的内置层，从 Dense 到 Conv2D，再到 LSTM，再到更高级的层，如 Conv3DTranspose 或 ConvLSTM2D。 明智地重用内置功能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-custody",
   "metadata": {},
   "source": [
    "# 层权重创建\n",
    "`self.add_weight()` 方法为您提供了创建权重的快捷方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "instructional-removal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 17.7 ms (started: 2021-08-26 19:32:06 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "# Instantiate our lazy layer.\n",
    "linear_layer = Linear(4)\n",
    "\n",
    "# This will also call `build(input_shape)` and create the weights.\n",
    "y = linear_layer(tf.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-attachment",
   "metadata": {},
   "source": [
    "# 层梯度\n",
    "您可以通过在 `GradientTape` 中调用图层来自动检索图层权重的梯度。 使用这些梯度，您可以手动或使用优化器对象更新层的权重。 当然，如果需要，您可以在使用之前修改梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "simplified-atlanta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.419081926345825\n",
      "Step: 100 Loss: 2.3437397480010986\n",
      "Step: 200 Loss: 2.1802117824554443\n",
      "Step: 300 Loss: 2.0649428367614746\n",
      "Step: 400 Loss: 2.0387189388275146\n",
      "Step: 500 Loss: 1.9206409454345703\n",
      "Step: 600 Loss: 1.8478238582611084\n",
      "Step: 700 Loss: 1.6928911209106445\n",
      "Step: 800 Loss: 1.7000257968902588\n",
      "Step: 900 Loss: 1.6581981182098389\n",
      "time: 8.31 s (started: 2021-08-26 19:34:27 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Instantiate our linear layer (defined above) with 10 units.\n",
    "linear_layer = Linear(10)\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Iterate over the batches of the dataset.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "\n",
    "    # Open a GradientTape.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        logits = linear_layer(x)\n",
    "\n",
    "        # Loss value for this batch.\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "    # Get gradients of the loss wrt the weights.\n",
    "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-aging",
   "metadata": {},
   "source": [
    "# 可训练和不可训练的权重\n",
    "由层创建的权重可以是可训练的，也可以是不可训练的。 它们分别暴露在 `trainable_weights` 和 `non_trainable_weights` 中。 这是一个具有不可训练权重的层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "pursuant-microphone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n",
      "time: 15.8 ms (started: 2021-08-26 19:36:30 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    \"\"\"Returns the sum of the inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        # Create a non-trainable weight.\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [2. 2.]\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [4. 4.]\n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-liechtenstein",
   "metadata": {},
   "source": [
    "# 拥有层的层\n",
    "层可以递归嵌套以创建更大的计算块。 每个层将跟踪其子层（可训练和不可训练）的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "constitutional-vegetarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 23.7 ms (started: 2021-08-26 19:38:09 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 让我们重用 Linear 类\n",
    "# 使用我们上面定义的 `build` 方法。\n",
    "\n",
    "\n",
    "class MLP(keras.layers.Layer):\n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "# The first call to the `mlp` object will create the weights.\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# Weights are recursively tracked.\n",
    "assert len(mlp.weights) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-knock",
   "metadata": {},
   "source": [
    "请注意，我们上面手动创建的 MLP 等效于以下内置选项："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "liquid-coating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2021-08-26 19:38:37 +08:00)\n"
     ]
    }
   ],
   "source": [
    "mlp = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-provision",
   "metadata": {},
   "source": [
    "# 跟踪层造成的损失\n",
    "层可以通过`add_loss()` 方法在前向传递期间产生损失。 这对于正则化损失特别有用。 子层创建的损失由父层递归跟踪。\n",
    "\n",
    "这是一个创建活动正则化损失的层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bronze-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.1 ms (started: 2021-08-26 19:41:55 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class ActivityRegularization(keras.layers.Layer):\n",
    "    \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularization, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # We use `add_loss` to create a regularization loss\n",
    "        # that depends on the inputs.\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-yacht",
   "metadata": {},
   "source": [
    "任何包含该层的模型都将跟踪此正则化损失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "conventional-musician",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.20129697>]\n",
      "time: 21.4 ms (started: 2021-08-26 19:43:58 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Let's use the loss layer in a MLP block.\n",
    "\n",
    "\n",
    "class SparseMLP(keras.layers.Layer):\n",
    "    \"\"\"Stack of Linear layers with a sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SparseMLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.regularization = ActivityRegularization(1e-2)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.regularization(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = SparseMLP()\n",
    "y = mlp(tf.ones((10, 10)))\n",
    "\n",
    "print(mlp.losses)  # List containing one float32 scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-african",
   "metadata": {},
   "source": [
    "这些损失在每次前向传递开始时由顶层清除——它们不会累积。 layer.losses 总是只包含在最后一次向前传递期间产生的损失。 在编写训练循环时，您通常会在计算梯度之前通过对它们求和来使用这些损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "incredible-certification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 5.366790771484375\n",
      "Step: 100 Loss: 2.538097381591797\n",
      "Step: 200 Loss: 2.395237922668457\n",
      "Step: 300 Loss: 2.3674986362457275\n",
      "Step: 400 Loss: 2.3502919673919678\n",
      "Step: 500 Loss: 2.3570244312286377\n",
      "Step: 600 Loss: 2.326946258544922\n",
      "Step: 700 Loss: 2.3309104442596436\n",
      "Step: 800 Loss: 2.314706802368164\n",
      "Step: 900 Loss: 2.3405709266662598\n",
      "time: 9.93 s (started: 2021-08-26 19:46:50 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Losses correspond to the *last* forward pass.\n",
    "mlp = SparseMLP()\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1  # No accumulation.\n",
    "\n",
    "# Let's demonstrate how to use these losses in a training loop.\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# A new MLP.\n",
    "mlp = SparseMLP()\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        logits = mlp(x)\n",
    "\n",
    "        # 该批次的外部损失值。\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "        # 添加前向传递期间产生的损失。\n",
    "        loss += sum(mlp.losses)\n",
    "\n",
    "        # 获取权重的损失梯度。\n",
    "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "\n",
    "    # 更新我们的线性层的权重。\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-stylus",
   "metadata": {},
   "source": [
    "# 跟踪衡量指标\n",
    "Keras 提供了广泛的内置指标，例如 `tf.keras.metrics.AUC` 或 `tf.keras.metrics.PrecisionAtRecall`。 用几行代码创建自己的指标也很容易。\n",
    "\n",
    "要在自定义训练循环中使用指标，您需要：\n",
    "* 实例化度量对象，例如 `metric = tf.keras.metrics.AUC()`\n",
    "* 为每批数据调用其 `metric.udpate_state(targets, predictions)` 方法\n",
    "* 通过 `metric.result()` 查询其结果\n",
    "* 通过 `metric.reset_states()` 在 epoch 结束或评估开始时重置指标的状态\n",
    "\n",
    "这是一个简单的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "liable-austin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 0\n",
      "Total running accuracy so far: 0.031\n",
      "Epoch: 0 Step: 200\n",
      "Total running accuracy so far: 0.772\n",
      "Epoch: 0 Step: 400\n",
      "Total running accuracy so far: 0.837\n",
      "Epoch: 0 Step: 600\n",
      "Total running accuracy so far: 0.863\n",
      "Epoch: 0 Step: 800\n",
      "Total running accuracy so far: 0.878\n",
      "Epoch: 1 Step: 0\n",
      "Total running accuracy so far: 0.938\n",
      "Epoch: 1 Step: 200\n",
      "Total running accuracy so far: 0.940\n",
      "Epoch: 1 Step: 400\n",
      "Total running accuracy so far: 0.940\n",
      "Epoch: 1 Step: 600\n",
      "Total running accuracy so far: 0.941\n",
      "Epoch: 1 Step: 800\n",
      "Total running accuracy so far: 0.942\n",
      "time: 23.7 s (started: 2021-08-26 19:53:45 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 实例化一个度量对象\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# 准备我们的层、损失和优化器。\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(2):\n",
    "    # Iterate over the batches of a dataset.\n",
    "    for step, (x, y) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            # Compute the loss value for this batch.\n",
    "            loss_value = loss_fn(y, logits)\n",
    "\n",
    "        # 更新“准确度”指标的状态。\n",
    "        accuracy.update_state(y, logits)\n",
    "\n",
    "        # 更新模型的权重以最小化损失值。\n",
    "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "        # 记录目前的准确度值。\n",
    "        if step % 200 == 0:\n",
    "            print(\"Epoch:\", epoch, \"Step:\", step)\n",
    "            print(\"Total running accuracy so far: %.3f\" % accuracy.result())\n",
    "\n",
    "    # 在Epoch结束时重置指标的状态\n",
    "    accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-moldova",
   "metadata": {},
   "source": [
    "除此之外，类似于 `self.add_loss()` 方法，您可以访问层上的 `self.add_metric()` 方法。 它跟踪您传递给它的任何数量的平均值。 您可以通过在任何层或模型上调用 `layer.reset_metrics()` 来重置这些指标的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-botswana",
   "metadata": {},
   "source": [
    "## 编译函数\n",
    "急切地运行非常适合调试，但通过将计算编译为静态图，您将获得更好的性能。 静态图是研究人员最好的朋友。 您可以通过将其包装在 tf.function 装饰器中来编译任何函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "focused-processor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 2.329501152038574\n",
      "Step: 100 Loss: 0.644128680229187\n",
      "Step: 200 Loss: 0.5418049693107605\n",
      "Step: 300 Loss: 0.35992997884750366\n",
      "Step: 400 Loss: 0.34068578481674194\n",
      "Step: 500 Loss: 0.19066256284713745\n",
      "Step: 600 Loss: 0.21403372287750244\n",
      "Step: 700 Loss: 0.2896679937839508\n",
      "Step: 800 Loss: 0.37784165143966675\n",
      "Step: 900 Loss: 0.34159013628959656\n",
      "time: 3.6 s (started: 2021-08-26 20:13:56 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Prepare our layer, loss, and optimizer.\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(32, activation=\"relu\"),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Create a training step function.\n",
    "\n",
    "\n",
    "@tf.function  # Make it fast.\n",
    "def train_on_batch(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(y, logits)\n",
    "#         gradients = tape.gradient(loss, model.trainable_weights)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "    optimizer.minimize(loss, model.trainable_weights, tape=tape)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_on_batch(x, y)\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-metabolism",
   "metadata": {},
   "source": [
    "# 训练模式&推理模式\n",
    "某些层，尤其是 BatchNormalization 层和 Dropout 层，在训练和推理过程中具有不同的行为。 对于此类层，标准做法是在 call 方法中公开`training`（布尔值）参数。\n",
    "\n",
    "通过在调用中公开此参数，您可以启用内置的训练和评估循环（例如 fit）以在训练和推理模式中正确使用该层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "specific-antigua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.9 ms (started: 2021-08-26 19:57:41 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Dropout(keras.layers.Layer):\n",
    "    def __init__(self, rate):\n",
    "        super(Dropout, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs\n",
    "\n",
    "\n",
    "class MLPWithDropout(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLPWithDropout()\n",
    "y_train = mlp(tf.ones((2, 2)), training=True)\n",
    "y_test = mlp(tf.ones((2, 2)), training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-technical",
   "metadata": {},
   "source": [
    "# 用于模型构建的函数式 API\n",
    "要构建深度学习模型，您不必一直使用面向对象的编程。 到目前为止，我们看到的所有层也可以按功能组合，如下所示（我们称之为“函数 API”）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "young-emperor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 165 ms (started: 2021-08-26 19:59:53 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# 我们使用 `Input` 对象来描述输入的形状和数据类型。\n",
    "# 这是深度学习相当于*声明一个类型*。\n",
    "# 形状参数是每个样本； 它不包括批量大小。\n",
    "# 函数式 API 专注于定义每个样本的转换。\n",
    "# 我们创建的模型将自动批处理每个样本的转换，以便可以在批量数据上调用它。\n",
    "inputs = tf.keras.Input(shape=(16,), dtype=\"float32\")\n",
    "\n",
    "# 我们在这些“类型”对象上调用层\n",
    "# 并且它们返回更新的类型（新的形状/数据类型）。\n",
    "x = Linear(32)(inputs)  # 我们正在重用我们之前定义的线性层。\n",
    "x = Dropout(0.5)(x)  # 我们正在重用我们之前定义的 Dropout 层。\n",
    "outputs = Linear(10)(x)\n",
    "\n",
    "# 可以通过指定输入和输出来定义功能性`Model`。\n",
    "# 模型本身就是一个层，就像任何其他层一样。\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# 在对任何数据调用之前，功能模型已经具有权重。\n",
    "# 那是因为我们预先定义了它的输入形状（在`Input`中）。\n",
    "assert len(model.weights) == 4\n",
    "\n",
    "# 为了好玩，让我们在一些数据上调用我们的模型。\n",
    "y = model(tf.ones((2, 16)))\n",
    "assert y.shape == (2, 10)\n",
    "\n",
    "# 你可以在 `__call__` 中传递一个 `training` 参数\n",
    "#（它将被传递到 Dropout 层）。\n",
    "y = model(tf.ones((2, 16)), training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-melbourne",
   "metadata": {},
   "source": [
    "函数式 API 往往比子类化更简洁，并提供了一些其他优势（与非类型化 OO 开发相比，函数式类型语言提供的优势通常相同）。 但是，它只能用于定义层的 DAG——递归网络应该被定义为层子类。\n",
    "\n",
    "在此处了解有关 Functional API 的更多信息。\n",
    "\n",
    "在您的研究工作流程中，您可能经常发现自己混合搭配了面向对象模型和功能模型。\n",
    "\n",
    "请注意，模型类还具有内置的训练和评估循环（fit() 和evaluate()）。 如果您想将这些循环用于您的 OO 模型，您始终可以对 Model 类进行子类化（它的工作方式与子类化 Layer 完全一样）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-midnight",
   "metadata": {},
   "source": [
    "# 端到端实验示例 1：变分自编码器。\n",
    "以下是您迄今为止学到的一些内容：\n",
    "* 一个层封装了一个状态（在 `__init__` 或 `build` 中创建）和一些计算（在调用中定义）。\n",
    "* 层可以递归嵌套以创建新的、更大的计算块。\n",
    "* 您可以通过打开 `GradientTape`，在磁带范围内调用您的模型，然后检索梯度并通过优化器应用它们来轻松编写高度可破解的训练循环。\n",
    "* 您可以使用 `@tf.function` 装饰器加速训练循环。\n",
    "* 层可以通过 `self.add_loss()` 创建和跟踪损失（通常是正则化损失）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-inspiration",
   "metadata": {},
   "source": [
    "让我们把所有这些东西放在一个端到端的例子中：我们将实现一个变分自动编码器 (VAE)。 我们将在 MNIST 数字上训练它。\n",
    "\n",
    "我们的 VAE 将是 Layer 的子类，构建为子类 Layer 的嵌套层组合。 它将具有正则化损失（KL 散度）。\n",
    "\n",
    "下面是我们的模型定义。\n",
    "\n",
    "首先，我们有一个 Encoder 类，它使用采样层将 MNIST 数字映射到潜在空间三元组 `(z_mean, z_log_var, z)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "peripheral-czech",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.1 ms (started: 2021-08-26 20:41:26 +08:00)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-hydrogen",
   "metadata": {},
   "source": [
    "接下来，我们有一个解码器类，它将概率潜在空间坐标映射回 MNIST 数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "wrapped-decade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 700 µs (started: 2021-08-26 20:16:03 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
    "        self.dense_output = layers.Dense(original_dim, activation=tf.nn.sigmoid)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-makeup",
   "metadata": {},
   "source": [
    "最后，我们的 VariationalAutoEncoder 将编码器和解码器组合在一起，并通过 add_loss() 创建 KL 散度正则化损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "actual-jerusalem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.18 ms (started: 2021-08-26 20:37:04 +08:00)\n"
     ]
    }
   ],
   "source": [
    "class VariationalAutoEncoder(layers.Layer):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(**kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
    "        )\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-attachment",
   "metadata": {},
   "source": [
    "现在，让我们编写一个训练循环。 我们的训练步骤用 @tf.function 修饰以编译成一个超快速的图函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "overall-plant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 0.3442484736442566\n",
      "Step: 100 Loss: 0.1264608879343118\n",
      "Step: 200 Loss: 0.10004165086580154\n",
      "Step: 300 Loss: 0.08995694439375519\n",
      "Step: 400 Loss: 0.0848816904399609\n",
      "Step: 500 Loss: 0.0816401359132545\n",
      "Step: 600 Loss: 0.07925445616815531\n",
      "Step: 700 Loss: 0.07792138239732822\n",
      "Step: 800 Loss: 0.07670980008707362\n",
      "Step: 900 Loss: 0.07570658301672052\n",
      "Step: 1000 Loss: 0.07479102916635952\n",
      "time: 3.91 s (started: 2021-08-26 20:41:37 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Our model.\n",
    "vae = VariationalAutoEncoder(original_dim=784, intermediate_dim=64, latent_dim=32)\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def training_step(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        reconstructed = vae(x)  # Compute input reconstruction.\n",
    "        # Compute loss.\n",
    "        loss = loss_fn(x, reconstructed)\n",
    "        loss += sum(vae.losses)  # Add KLD term.\n",
    "    # Update the weights of the VAE.\n",
    "#     grads = tape.gradient(loss, vae.trainable_weights)\n",
    "#     optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "    optimizer.minimize(loss, vae.trainable_weights, tape=tape)\n",
    "    return loss\n",
    "\n",
    "losses = []  # Keep track of the losses over time.\n",
    "for step, x in enumerate(dataset):\n",
    "    loss = training_step(x)\n",
    "    # Logging.\n",
    "    losses.append(float(loss))\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
    "\n",
    "    # Stop after 1000 steps.\n",
    "    # Training the model to convergence is left\n",
    "    # as an exercise to the reader.\n",
    "    if step >= 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-demand",
   "metadata": {},
   "source": [
    "如您所见，在 Keras 中构建和训练这种类型的模型是快速且轻松的。\n",
    "\n",
    "现在，您可能会发现上面的代码有些冗长：我们自己手动处理每一个小细节。 这提供了最大的灵活性，但也需要一些工作。\n",
    "\n",
    "让我们来看看我们的 VAE 的 Functional API 版本是什么样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "compressed-parker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 116 ms (started: 2021-08-26 20:42:31 +08:00)\n"
     ]
    }
   ],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-inventory",
   "metadata": {},
   "source": [
    "简洁多了，对吧？\n",
    "\n",
    "顺便说一下，Keras 在其模型类（fit() 和evaluate()）上还具有内置的训练和评估循环。 一探究竟："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "better-olive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f0098383820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f0098383820> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f00bc15c100>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.3 s (started: 2021-08-26 20:43:00 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.map(lambda x: (x, x))  # Use x_train as both inputs & targets\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "# Configure the model for training.\n",
    "vae.compile(optimizer, loss=loss_fn)\n",
    "\n",
    "# Actually training the model.\n",
    "vae.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-hopkins",
   "metadata": {},
   "source": [
    "Functional API 和 fit 的使用将我们的示例从 65 行减少到 25 行（包括模型定义和训练）。 Keras 的理念是为您提供诸如此类的提高生产力的功能，同时使您能够自己编写所有内容，从而获得对每一个小细节的绝对控制。 就像我们在前两段的低级训练循环中所做的那样。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-little",
   "metadata": {},
   "source": [
    "# 端到端实验示例 2：超网络。\n",
    "让我们来看看另一种研究实验：超网络。\n",
    "\n",
    "这个想法是使用一个小的深度神经网络（超网络）来为一个更大的网络（主网络）生成权重。\n",
    "\n",
    "让我们实现一个非常简单的超网络：我们将使用一个小的 2 层网络来生成一个更大的 3 层网络的权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "seeing-obligation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.2 ms (started: 2021-08-26 20:47:07 +08:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_dim = 784\n",
    "classes = 10\n",
    "\n",
    "# 这是我们实际用于预测标签的主要网络。\n",
    "main_network = keras.Sequential(\n",
    "    [keras.layers.Dense(64, activation=tf.nn.relu), keras.layers.Dense(classes),]\n",
    ")\n",
    "\n",
    "# 它不需要创建自己的权重，所以让我们将它的层标记为已经构建。 这样，调用 `main_network` 不会创建新变量。\n",
    "for layer in main_network.layers:\n",
    "    layer.built = True\n",
    "\n",
    "# 这是要生成的权重系数的数量。 主网络中的每一层都需要 output_dim * input_dim + output_dim 系数。\n",
    "num_weights_to_generate = (classes * 64 + classes) + (64 * input_dim + 64)\n",
    "\n",
    "# 这是生成上面`main_network` 权重的超网络。\n",
    "hypernetwork = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(num_weights_to_generate, activation=tf.nn.sigmoid),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-assets",
   "metadata": {},
   "source": [
    "这是我们的训练循环。 对于每批数据：\n",
    "* 我们使用超网络生成一组权重系数，weights_pred\n",
    "* 我们将这些系数重塑为 main_network 的内核和偏置张量\n",
    "* 我们运行 main_network 的前向传递来计算实际的 MNIST 预测\n",
    "* 我们通过超网络的权重运行反向传播以最小化最终分类损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "contrary-newspaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 3.984729766845703\n",
      "Step: 100 Loss: 2.755797770685784\n",
      "Step: 200 Loss: 2.293525581169336\n",
      "Step: 300 Loss: 2.1120521302282365\n",
      "Step: 400 Loss: 1.9611971037511127\n",
      "Step: 500 Loss: 1.8432813464796836\n",
      "Step: 600 Loss: 1.71873571755215\n",
      "Step: 700 Loss: 1.6866467812311818\n",
      "Step: 800 Loss: 1.6358893030762025\n",
      "Step: 900 Loss: 1.5616319959051859\n",
      "Step: 1000 Loss: 1.4955019258477595\n",
      "time: 6.2 s (started: 2021-08-26 20:49:34 +08:00)\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "\n",
    "# We'll use a batch size of 1 for this experiment.\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(1)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict weights for the outer model.\n",
    "        weights_pred = hypernetwork(x)\n",
    "\n",
    "        # Reshape them to the expected shapes for w and b for the outer model.\n",
    "        # Layer 0 kernel.\n",
    "        start_index = 0\n",
    "        w0_shape = (input_dim, 64)\n",
    "        w0_coeffs = weights_pred[:, start_index : start_index + np.prod(w0_shape)]\n",
    "        w0 = tf.reshape(w0_coeffs, w0_shape)\n",
    "        start_index += np.prod(w0_shape)\n",
    "        # Layer 0 bias.\n",
    "        b0_shape = (64,)\n",
    "        b0_coeffs = weights_pred[:, start_index : start_index + np.prod(b0_shape)]\n",
    "        b0 = tf.reshape(b0_coeffs, b0_shape)\n",
    "        start_index += np.prod(b0_shape)\n",
    "        # Layer 1 kernel.\n",
    "        w1_shape = (64, classes)\n",
    "        w1_coeffs = weights_pred[:, start_index : start_index + np.prod(w1_shape)]\n",
    "        w1 = tf.reshape(w1_coeffs, w1_shape)\n",
    "        start_index += np.prod(w1_shape)\n",
    "        # Layer 1 bias.\n",
    "        b1_shape = (classes,)\n",
    "        b1_coeffs = weights_pred[:, start_index : start_index + np.prod(b1_shape)]\n",
    "        b1 = tf.reshape(b1_coeffs, b1_shape)\n",
    "        start_index += np.prod(b1_shape)\n",
    "\n",
    "        # Set the weight predictions as the weight variables on the outer model.\n",
    "        main_network.layers[0].kernel = w0\n",
    "        main_network.layers[0].bias = b0\n",
    "        main_network.layers[1].kernel = w1\n",
    "        main_network.layers[1].bias = b1\n",
    "\n",
    "        # Inference on the outer model.\n",
    "        preds = main_network(x)\n",
    "        loss = loss_fn(y, preds)\n",
    "\n",
    "    # Train only inner model.\n",
    "#     grads = tape.gradient(loss, hypernetwork.trainable_weights)\n",
    "#     optimizer.apply_gradients(zip(grads, hypernetwork.trainable_weights))\n",
    "    optimizer.minimize(loss, hypernetwork.trainable_weights, tape = tape)\n",
    "    return loss\n",
    "\n",
    "losses = []  # Keep track of the losses over time.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_step(x, y)\n",
    "\n",
    "    # Logging.\n",
    "    losses.append(float(loss))\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
    "\n",
    "    # Stop after 1000 steps.\n",
    "    # Training the model to convergence is left\n",
    "    # as an exercise to the reader.\n",
    "    if step >= 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-currency",
   "metadata": {},
   "source": [
    "使用 Keras 实现任意研究想法既简单又高效。 想象一下每天尝试 25 个想法（平均每个实验 20 分钟）！\n",
    "\n",
    "Keras 旨在尽快将想法转化为结果，因为我们相信这是开展出色研究的关键。\n",
    "\n",
    "我们希望您喜欢这个快速介绍。 让我们知道您使用 Keras 构建了什么！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "hollywood-device",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.57 ms (started: 2021-08-26 20:50:34 +08:00)\n"
     ]
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-alloy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
